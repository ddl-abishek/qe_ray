{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e69337-539b-476a-8dac-cc9f53a74ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Dict\n",
    "from ray.air import session\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import ray.train as train\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.air.config import ScalingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c513af2b-08ae-4753-aa09-ce8384310634",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Download training data from open datasets.\n",
    "# training_data = datasets.FashionMNIST(\n",
    "#     root=\"~/data\",\n",
    "#     train=True,\n",
    "#     download=True,\n",
    "#     transform=ToTensor(),\n",
    "# )\n",
    "\n",
    "# # Download test data from open datasets.\n",
    "# test_data = datasets.FashionMNIST(\n",
    "#     root=\"~/data\",\n",
    "#     train=False,\n",
    "#     download=True,\n",
    "#     transform=ToTensor(),\n",
    "# )\n",
    "def load_data():\n",
    "    # Download training data from open datasets.\n",
    "    training_data = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "    # Download test data from open datasets.\n",
    "    test_data = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "    return training_data, test_data\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_epoch(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset) // session.get_world_size()\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def validate_epoch(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset) // session.get_world_size()\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(\n",
    "        f\"Test Error: \\n \"\n",
    "        f\"Accuracy: {(100 * correct):>0.1f}%, \"\n",
    "        f\"Avg loss: {test_loss:>8f} \\n\"\n",
    "    )\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "def train_func(config: Dict):\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    lr = config[\"lr\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "\n",
    "    worker_batch_size = batch_size // session.get_world_size()\n",
    "\n",
    "    # Create data loaders.\n",
    "    training_data, test_data = load_data()  # <- this is new!\n",
    "    train_dataloader = DataLoader(training_data, batch_size=worker_batch_size)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=worker_batch_size)\n",
    "\n",
    "    train_dataloader = train.torch.prepare_data_loader(train_dataloader)\n",
    "    test_dataloader = train.torch.prepare_data_loader(test_dataloader)\n",
    "\n",
    "    # Create model.\n",
    "    model = NeuralNetwork()\n",
    "    model = train.torch.prepare_model(model)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        train_epoch(train_dataloader, model, loss_fn, optimizer)\n",
    "        loss = validate_epoch(test_dataloader, model, loss_fn)\n",
    "        session.report(dict(loss=loss))\n",
    "\n",
    "\n",
    "def train_fashion_mnist(num_workers=3, use_gpu=True):\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker=train_func,\n",
    "        train_loop_config={\"lr\": 1e-3, \"batch_size\": 64, \"epochs\": 4},\n",
    "        scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=use_gpu),\n",
    "    )\n",
    "    result = trainer.fit()\n",
    "    print(f\"Last result: {result.metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "407a98c3-19fa-46b8-970b-76d1412176c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m Current time: 2023-04-05 13:58:25 (running for 00:00:02.43)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m Memory usage on this node: 3.7/31.0 GiB\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m Resources requested: 1.0/8 CPUs, 2.0/4 GPUs, 0.0/26.63 GiB heap, 0.0/11.83 GiB objects (0.0/4.0 accelerator_type:A10G)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m Result logdir: /home/ray/ray_results/TorchTrainer_2023-04-05_13-58-23\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m +--------------------------+----------+----------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m | Trial name               | status   | loc            |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m |--------------------------+----------+----------------|\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m | TorchTrainer_9a55c_00000 | RUNNING  | 10.0.57.80:336 |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m +--------------------------+----------+----------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m Result for TorchTrainer_9a55c_00000:\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   date: 2023-04-05_13-58-25\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   experiment_id: c4bc2b6458dd4ef58617b080a1f6a8e5\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   hostname: ray-642dd2d153943c280b964b6c-ray-worker-0\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   node_ip: 10.0.57.80\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   pid: 336\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   timestamp: 1680728305\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   trial_id: 9a55c_00000\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m Current time: 2023-04-05 13:58:27 (running for 00:00:04.24)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m Memory usage on this node: 3.7/31.0 GiB\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m Resources requested: 0/8 CPUs, 0/4 GPUs, 0.0/26.63 GiB heap, 0.0/11.83 GiB objects (0.0/4.0 accelerator_type:A10G)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m Result logdir: /home/ray/ray_results/TorchTrainer_2023-04-05_13-58-23\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m Number of trials: 1/1 (1 ERROR)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m +--------------------------+----------+----------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m | Trial name               | status   | loc            |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m |--------------------------+----------+----------------|\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m | TorchTrainer_9a55c_00000 | ERROR    | 10.0.57.80:336 |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m +--------------------------+----------+----------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m Number of errored trials: 1\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m +--------------------------+--------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m | Trial name               |   # failures | error file                                                                                                      |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m |--------------------------+--------------+-----------------------------------------------------------------------------------------------------------------|\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m | TorchTrainer_9a55c_00000 |            1 | /home/ray/ray_results/TorchTrainer_2023-04-05_13-58-23/TorchTrainer_9a55c_00000_0_2023-04-05_13-58-24/error.txt |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m +--------------------------+--------------+-----------------------------------------------------------------------------------------------------------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m \n",
      "Last result: {'trial_id': '9a55c_00000'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=790, ip=10.0.40.106)\u001b[0m 2023-04-05 13:58:27,507\tINFO config.py:71 -- Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m 2023-04-05 13:58:27,501\tERROR function_trainable.py:298 -- Runner Thread raised error.\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 289, in run\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 362, in entrypoint\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m     return self._trainable_func(\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 466, in _resume_span\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/train/base_trainer.py\", line 460, in _trainable_func\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 684, in _trainable_func\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/train/base_trainer.py\", line 375, in train_func\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/data_parallel_trainer.py\", line 346, in training_loop\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m     backend_executor.start(initialization_hook=None)\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/_internal/backend_executor.py\", line 126, in start\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m     self._backend.on_start(self.worker_group, self._backend_config)\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/torch/config.py\", line 152, in on_start\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m     ray.get(setup_futures)\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py\", line 2275, in get\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m     raise value.as_instanceof_cause()\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::RayTrainWorker._RayTrainWorker__execute()\u001b[39m (pid=365, ip=10.0.57.80, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f659d772e50>)\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/_internal/worker_group.py\", line 26, in __execute\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/torch/config.py\", line 89, in _setup_torch_process_group\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m     dist.init_process_group(\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py\", line 500, in init_process_group\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m     store, rank, world_size = next(rendezvous_iterator)\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/torch/distributed/rendezvous.py\", line 190, in _env_rendezvous_handler\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m     store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)\n",
      "\u001b[2m\u001b[36m(TorchTrainer pid=336, ip=10.0.57.80)\u001b[0m RuntimeError: Connection reset by peer\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m 2023-04-05 13:58:27,616\tERROR trial_runner.py:980 -- Trial TorchTrainer_9a55c_00000: Error processing event.\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::TrainTrainable.train()\u001b[39m (pid=336, ip=10.0.57.80, repr=TorchTrainer)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 347, in train\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 417, in step\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m     self._report_thread_runner_error(block=True)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 589, in _report_thread_runner_error\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 289, in run\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m     self._entrypoint()\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 362, in entrypoint\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m     return self._trainable_func(\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/train/base_trainer.py\", line 460, in _trainable_func\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 684, in _trainable_func\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   File \"/opt/conda/lib/python3.9/site-packages/ray/train/base_trainer.py\", line 375, in train_func\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/data_parallel_trainer.py\", line 346, in training_loop\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m     backend_executor.start(initialization_hook=None)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/_internal/backend_executor.py\", line 126, in start\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m     self._backend.on_start(self.worker_group, self._backend_config)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/torch/config.py\", line 152, in on_start\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m     ray.get(setup_futures)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m ray.exceptions.RayTaskError(RuntimeError): \u001b[36mray::RayTrainWorker._RayTrainWorker__execute()\u001b[39m (pid=365, ip=10.0.57.80, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7f659d772e50>)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/_internal/worker_group.py\", line 26, in __execute\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/ray/train/torch/config.py\", line 89, in _setup_torch_process_group\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m     dist.init_process_group(\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py\", line 500, in init_process_group\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m     store, rank, world_size = next(rendezvous_iterator)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m   File \"/home/ray/anaconda3/lib/python3.9/site-packages/torch/distributed/rendezvous.py\", line 190, in _env_rendezvous_handler\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m     store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m RuntimeError: Connection reset by peer\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m 2023-04-05 13:58:27,724\tERROR tune.py:754 -- Trials did not complete: [TorchTrainer_9a55c_00000]\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=2185)\u001b[0m 2023-04-05 13:58:27,724\tINFO tune.py:758 -- Total run time: 4.41 seconds (4.24 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--address\", required=False, type=str, help=\"the address to use for Ray\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-workers\",\n",
    "        \"-n\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        help=\"Sets number of workers for training.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use-gpu\", action=\"store_true\", default=True, help=\"Enables GPU training\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--smoke-test\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Finish quickly for testing.\",\n",
    "    )\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    import ray\n",
    "    if ray.is_initialized() == False:\n",
    "        print(\"Connecting to Ray cluster...\")\n",
    "        service_host = os.environ[\"RAY_HEAD_SERVICE_HOST\"]\n",
    "        service_port = os.environ[\"RAY_HEAD_SERVICE_PORT\"]\n",
    "        ray.util.connect(f\"{service_host}:{service_port}\")\n",
    "\n",
    "\n",
    "    # if args.smoke_test:\n",
    "    #     # 2 workers + 1 for trainer.\n",
    "    #     ray.init(num_cpus=3)\n",
    "    #     train_fashion_mnist()\n",
    "    # else:\n",
    "    # ray.init(address=args.address)\n",
    "    train_fashion_mnist(num_workers=args.num_workers, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b3bce2-55d7-4425-9023-64228886c9d7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TorchTrainer in module ray.train.torch.torch_trainer:\n",
      "\n",
      "class TorchTrainer(ray.train.data_parallel_trainer.DataParallelTrainer)\n",
      " |  TorchTrainer(*args, **kwargs)\n",
      " |  \n",
      " |  A Trainer for data parallel PyTorch training.\n",
      " |  \n",
      " |  This Trainer runs the function ``train_loop_per_worker`` on multiple Ray\n",
      " |  Actors. These actors already have the necessary torch process group already\n",
      " |  configured for distributed PyTorch training.\n",
      " |  \n",
      " |  The ``train_loop_per_worker`` function is expected to take in either 0 or 1\n",
      " |  arguments:\n",
      " |  \n",
      " |  .. code-block:: python\n",
      " |  \n",
      " |      def train_loop_per_worker():\n",
      " |          ...\n",
      " |  \n",
      " |  .. code-block:: python\n",
      " |  \n",
      " |      def train_loop_per_worker(config: Dict):\n",
      " |          ...\n",
      " |  \n",
      " |  If ``train_loop_per_worker`` accepts an argument, then\n",
      " |  ``train_loop_config`` will be passed in as the argument. This is useful if you\n",
      " |  want to tune the values in ``train_loop_config`` as hyperparameters.\n",
      " |  \n",
      " |  If the ``datasets`` dict contains a training dataset (denoted by\n",
      " |  the \"train\" key), then it will be split into multiple dataset\n",
      " |  shards that can then be accessed by ``session.get_dataset_shard(\"train\")`` inside\n",
      " |  ``train_loop_per_worker``. All the other datasets will not be split and\n",
      " |  ``session.get_dataset_shard(...)`` will return the the entire Dataset.\n",
      " |  \n",
      " |  Inside the ``train_loop_per_worker`` function, you can use any of the\n",
      " |  :ref:`Ray AIR session methods <air-session-ref>`.\n",
      " |  \n",
      " |  .. code-block:: python\n",
      " |  \n",
      " |      def train_loop_per_worker():\n",
      " |          # Report intermediate results for callbacks or logging and\n",
      " |          # checkpoint data.\n",
      " |          session.report(...)\n",
      " |  \n",
      " |          # Returns dict of last saved checkpoint.\n",
      " |          session.get_checkpoint()\n",
      " |  \n",
      " |          # Returns the Ray Dataset shard for the given key.\n",
      " |          session.get_dataset_shard(\"my_dataset\")\n",
      " |  \n",
      " |          # Returns the total number of workers executing training.\n",
      " |          session.get_world_size()\n",
      " |  \n",
      " |          # Returns the rank of this worker.\n",
      " |          session.get_world_rank()\n",
      " |  \n",
      " |          # Returns the rank of the worker on the current node.\n",
      " |          session.get_local_rank()\n",
      " |  \n",
      " |  You can also use any of the Torch specific function utils,\n",
      " |  such as :func:`ray.train.torch.get_device` and :func:`ray.train.torch.prepare_model`\n",
      " |  \n",
      " |  .. code-block:: python\n",
      " |  \n",
      " |      def train_loop_per_worker():\n",
      " |          # Prepares model for distribted training by wrapping in\n",
      " |          # `DistributedDataParallel` and moving to correct device.\n",
      " |          train.torch.prepare_model(...)\n",
      " |  \n",
      " |          # Configures the dataloader for distributed training by adding a\n",
      " |          # `DistributedSampler`.\n",
      " |          # You should NOT use this if you are doing\n",
      " |          # `session.get_dataset_shard(...).iter_torch_batches(...)`\n",
      " |          train.torch.prepare_data_loader(...)\n",
      " |  \n",
      " |          # Returns the current torch device.\n",
      " |          train.torch.get_device()\n",
      " |  \n",
      " |  Any returns from the ``train_loop_per_worker`` will be discarded and not\n",
      " |  used or persisted anywhere.\n",
      " |  \n",
      " |  To save a model to use for the ``TorchPredictor``, you must save it under the\n",
      " |  \"model\" kwarg in ``Checkpoint`` passed to ``session.report()``.\n",
      " |  \n",
      " |  Example:\n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          import torch\n",
      " |          import torch.nn as nn\n",
      " |  \n",
      " |          import ray\n",
      " |          from ray import train\n",
      " |          from ray.air import session, Checkpoint\n",
      " |          from ray.train.torch import TorchTrainer\n",
      " |          from ray.air.config import ScalingConfig\n",
      " |  \n",
      " |          input_size = 1\n",
      " |          layer_size = 15\n",
      " |          output_size = 1\n",
      " |          num_epochs = 3\n",
      " |  \n",
      " |          class NeuralNetwork(nn.Module):\n",
      " |              def __init__(self):\n",
      " |                  super(NeuralNetwork, self).__init__()\n",
      " |                  self.layer1 = nn.Linear(input_size, layer_size)\n",
      " |                  self.relu = nn.ReLU()\n",
      " |                  self.layer2 = nn.Linear(layer_size, output_size)\n",
      " |  \n",
      " |              def forward(self, input):\n",
      " |                  return self.layer2(self.relu(self.layer1(input)))\n",
      " |  \n",
      " |          def train_loop_per_worker():\n",
      " |              dataset_shard = session.get_dataset_shard(\"train\")\n",
      " |              model = NeuralNetwork()\n",
      " |              loss_fn = nn.MSELoss()\n",
      " |              optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
      " |  \n",
      " |              model = train.torch.prepare_model(model)\n",
      " |  \n",
      " |              for epoch in range(num_epochs):\n",
      " |                  for batches in dataset_shard.iter_torch_batches(\n",
      " |                      batch_size=32, dtypes=torch.float\n",
      " |                  ):\n",
      " |                      inputs, labels = torch.unsqueeze(batches[\"x\"], 1), batches[\"y\"]\n",
      " |                      output = model(inputs)\n",
      " |                      loss = loss_fn(output, labels)\n",
      " |                      optimizer.zero_grad()\n",
      " |                      loss.backward()\n",
      " |                      optimizer.step()\n",
      " |                      print(f\"epoch: {epoch}, loss: {loss.item()}\")\n",
      " |  \n",
      " |                  session.report(\n",
      " |                      {},\n",
      " |                      checkpoint=Checkpoint.from_dict(\n",
      " |                          dict(epoch=epoch, model=model.state_dict())\n",
      " |                      ),\n",
      " |                  )\n",
      " |  \n",
      " |          train_dataset = ray.data.from_items(\n",
      " |              [{\"x\": x, \"y\": 2 * x + 1} for x in range(200)]\n",
      " |          )\n",
      " |          scaling_config = ScalingConfig(num_workers=3)\n",
      " |          # If using GPUs, use the below scaling config instead.\n",
      " |          # scaling_config = ScalingConfig(num_workers=3, use_gpu=True)\n",
      " |          trainer = TorchTrainer(\n",
      " |              train_loop_per_worker=train_loop_per_worker,\n",
      " |              scaling_config=scaling_config,\n",
      " |              datasets={\"train\": train_dataset})\n",
      " |          result = trainer.fit()\n",
      " |  \n",
      " |  Args:\n",
      " |      train_loop_per_worker: The training function to execute.\n",
      " |          This can either take in no arguments or a ``config`` dict.\n",
      " |      train_loop_config: Configurations to pass into\n",
      " |          ``train_loop_per_worker`` if it accepts an argument.\n",
      " |      torch_config: Configuration for setting up the PyTorch backend. If set to\n",
      " |          None, use the default configuration. This replaces the ``backend_config``\n",
      " |          arg of ``DataParallelTrainer``.\n",
      " |      scaling_config: Configuration for how to scale data parallel training.\n",
      " |      dataset_config: Configuration for dataset ingest.\n",
      " |      run_config: Configuration for the execution of the training run.\n",
      " |      datasets: Any Ray Datasets to use for training. Use\n",
      " |          the key \"train\" to denote which dataset is the training\n",
      " |          dataset. If a ``preprocessor`` is provided and has not already been fit,\n",
      " |          it will be fit on the training dataset. All datasets will be transformed\n",
      " |          by the ``preprocessor`` if one is provided.\n",
      " |      preprocessor: A ``ray.data.Preprocessor`` to preprocess the\n",
      " |          provided datasets.\n",
      " |      resume_from_checkpoint: A checkpoint to resume training from.\n",
      " |  \n",
      " |  PublicAPI (beta): This API is in beta and may change before becoming stable.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TorchTrainer\n",
      " |      ray.train.data_parallel_trainer.DataParallelTrainer\n",
      " |      ray.train.base_trainer.BaseTrainer\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, train_loop_per_worker: Union[Callable[[], NoneType], Callable[[Dict], NoneType]], *, train_loop_config: Optional[Dict] = None, torch_config: Optional[ray.train.torch.config.TorchConfig] = None, scaling_config: Optional[ray.air.config.ScalingConfig] = None, dataset_config: Optional[Dict[str, ray.air.config.DatasetConfig]] = None, run_config: Optional[ray.air.config.RunConfig] = None, datasets: Optional[Dict[str, Union[ForwardRef('Dataset'), Callable[[], ForwardRef('Dataset')]]]] = None, preprocessor: Optional[ForwardRef('Preprocessor')] = None, resume_from_checkpoint: Optional[ray.air.checkpoint.Checkpoint] = None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ray.train.data_parallel_trainer.DataParallelTrainer:\n",
      " |  \n",
      " |  get_dataset_config(self) -> Dict[str, ray.air.config.DatasetConfig]\n",
      " |      Return a copy of this Trainer's final dataset configs.\n",
      " |      \n",
      " |      Returns:\n",
      " |          The merged default + user-supplied dataset config.\n",
      " |  \n",
      " |  preprocess_datasets(self) -> None\n",
      " |      Called during fit() to preprocess dataset attributes with preprocessor.\n",
      " |      \n",
      " |      .. note:: This method is run on a remote process.\n",
      " |      \n",
      " |      This method is called prior to entering the training_loop.\n",
      " |      \n",
      " |      If the ``Trainer`` has both a datasets dict and\n",
      " |      a preprocessor, the datasets dict contains a training dataset (denoted by\n",
      " |      the \"train\" key), and the preprocessor has not yet\n",
      " |      been fit, then it will be fit on the train dataset.\n",
      " |      \n",
      " |      Then, all Trainer's datasets will be transformed by the preprocessor.\n",
      " |      \n",
      " |      The transformed datasets will be set back in the ``self.datasets`` attribute\n",
      " |      of the Trainer to be used when overriding ``training_loop``.\n",
      " |  \n",
      " |  training_loop(self) -> None\n",
      " |      Loop called by fit() to run training and report results to Tune.\n",
      " |      \n",
      " |      Note: this method runs on a remote process.\n",
      " |      \n",
      " |      ``self.datasets`` have already been preprocessed by ``self.preprocessor``.\n",
      " |      \n",
      " |      You can use the :ref:`Tune Function API functions <tune-function-docstring>`\n",
      " |      (``session.report()`` and ``session.get_checkpoint()``) inside\n",
      " |      this training loop.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      .. code-block: python\n",
      " |      \n",
      " |          from ray.train.trainer import BaseTrainer\n",
      " |      \n",
      " |          class MyTrainer(BaseTrainer):\n",
      " |              def training_loop(self):\n",
      " |                  for epoch_idx in range(5):\n",
      " |                      ...\n",
      " |                      session.report({\"epoch\": epoch_idx})\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from ray.train.data_parallel_trainer.DataParallelTrainer:\n",
      " |  \n",
      " |  __annotations__ = {'_checkpoint_manager_cls': typing.Type[ray.train._i...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ray.train.base_trainer.BaseTrainer:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  as_trainable(self) -> Type[ForwardRef('Trainable')]\n",
      " |      Convert self to a ``tune.Trainable`` class.\n",
      " |  \n",
      " |  fit(self) -> ray.air.result.Result\n",
      " |      Runs training.\n",
      " |      \n",
      " |          Returns:\n",
      " |              A Result object containing the training result.\n",
      " |      \n",
      " |          Raises:\n",
      " |              TrainingFailedError: If any failures during the execution of\n",
      " |              ``self.as_trainable()``.\n",
      " |          \n",
      " |      PublicAPI (beta): This API is in beta and may change before becoming stable.\n",
      " |  \n",
      " |  setup(self) -> None\n",
      " |      Called during fit() to perform initial setup on the Trainer.\n",
      " |      \n",
      " |      .. note:: This method is run on a remote process.\n",
      " |      \n",
      " |      This method will not be called on the driver, so any expensive setup\n",
      " |      operations should be placed here and not in ``__init__``.\n",
      " |      \n",
      " |      This method is called prior to ``preprocess_datasets`` and\n",
      " |      ``training_loop``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from ray.train.base_trainer.BaseTrainer:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from ray.train.base_trainer.BaseTrainer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TorchTrainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83db0fc1-7b67-4e2e-8ba9-2b5180883d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Parameters\n",
    "\n",
    "        train_loop_per_worker – The training function to execute. This can either take in no arguments or a config dict.\n",
    "\n",
    "        train_loop_config – Configurations to pass into train_loop_per_worker if it accepts an argument.\n",
    "\n",
    "        torch_config – Configuration for setting up the PyTorch backend. If set to None, use the default configuration. This replaces the backend_config arg of DataParallelTrainer.\n",
    "\n",
    "        scaling_config – Configuration for how to scale data parallel training.\n",
    "\n",
    "        dataset_config – Configuration for dataset ingest.\n",
    "\n",
    "        run_config – Configuration for the execution of the training run.\n",
    "\n",
    "        datasets – Any Ray Datasets to use for training. Use the key “train” to denote which dataset is the training dataset. If a preprocessor is provided and has not already been fit, it will be fit on the training dataset. All datasets will be transformed by the preprocessor if one is provided.\n",
    "\n",
    "        preprocessor – A ray.data.Preprocessor to preprocess the provided datasets.\n",
    "\n",
    "        resume_from_checkpoint – A checkpoint to resume training from.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2f649d-38e7-4102-852f-0fdb5df68647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
