{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3e69337-539b-476a-8dac-cc9f53a74ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Dict\n",
    "from ray.air import session\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import ray.train as train\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.air.config import ScalingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c513af2b-08ae-4753-aa09-ce8384310634",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /home/ubuntu/data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3ad747ba8c44cbad4b15c9af033081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/ubuntu/data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /home/ubuntu/data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /home/ubuntu/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d455b37f6b45f7a1869b639d6cffc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/ubuntu/data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /home/ubuntu/data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /home/ubuntu/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3170eb29314de8ab03556eb5295535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/ubuntu/data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /home/ubuntu/data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /home/ubuntu/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9632e4076aca42f28ce3a85f1431468a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/ubuntu/data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /home/ubuntu/data/FashionMNIST/raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.9/site-packages/torchvision/datasets/mnist.py:479: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"~/data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"~/data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_epoch(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset) // session.get_world_size()\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def validate_epoch(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset) // session.get_world_size()\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(\n",
    "        f\"Test Error: \\n \"\n",
    "        f\"Accuracy: {(100 * correct):>0.1f}%, \"\n",
    "        f\"Avg loss: {test_loss:>8f} \\n\"\n",
    "    )\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "def train_func(config: Dict):\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    lr = config[\"lr\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "\n",
    "    worker_batch_size = batch_size // session.get_world_size()\n",
    "\n",
    "    # Create data loaders.\n",
    "    train_dataloader = DataLoader(training_data, batch_size=worker_batch_size)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=worker_batch_size)\n",
    "\n",
    "    train_dataloader = train.torch.prepare_data_loader(train_dataloader)\n",
    "    test_dataloader = train.torch.prepare_data_loader(test_dataloader)\n",
    "\n",
    "    # Create model.\n",
    "    model = NeuralNetwork()\n",
    "    model = train.torch.prepare_model(model)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        train_epoch(train_dataloader, model, loss_fn, optimizer)\n",
    "        loss = validate_epoch(test_dataloader, model, loss_fn)\n",
    "        session.report(dict(loss=loss))\n",
    "\n",
    "\n",
    "def train_fashion_mnist(num_workers=3, use_gpu=True):\n",
    "    trainer = TorchTrainer(\n",
    "        train_loop_per_worker=train_func,\n",
    "        train_loop_config={\"lr\": 1e-3, \"batch_size\": 64, \"epochs\": 4},\n",
    "        scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=use_gpu),\n",
    "    )\n",
    "    result = trainer.fit()\n",
    "    print(f\"Last result: {result.metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "407a98c3-19fa-46b8-970b-76d1412176c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Ray cluster...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/ray/util/client/worker.py:597: UserWarning: More than 10MB of messages have been created to schedule tasks on the server. This can be slow on Ray Client due to communication overhead over the network. If you're running many fine-grained tasks, consider running them inside a single remote function. See the section on \"Too fine-grained tasks\" in the Ray Design Patterns document for more details: https://docs.google.com/document/d/167rnnDFIVRhHhK4mznEIemOtj63IOhtIPvSYaPgI4Fg/edit#heading=h.f7ins22n6nyl. If your functions frequently use large objects, consider storing the objects remotely with ray.put. An example of this is shown in the \"Closure capture of large / unserializable object\" section of the Ray Design Patterns document, available here: https://docs.google.com/document/d/167rnnDFIVRhHhK4mznEIemOtj63IOhtIPvSYaPgI4Fg/edit#heading=h.1afmymq455wu\n",
      "  warnings.warn(\n",
      "Warning: The actor TrainTrainable is very large (52 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Current time: 2023-04-05 12:16:23 (running for 00:00:03.78)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Memory usage on this node: 3.9/31.0 GiB\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Resources requested: 1.0/8 CPUs, 3.0/4 GPUs, 0.0/26.62 GiB heap, 0.0/11.82 GiB objects (0.0/4.0 accelerator_type:A10G)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Result logdir: /home/ray/ray_results/TorchTrainer_2023-04-05_12-16-18\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+----------+---------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | Trial name               | status   | loc           |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m |--------------------------+----------+---------------|\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | TorchTrainer_580a4_00000 | RUNNING  | 10.0.62.72:81 |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+----------+---------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m 2023-04-05 12:16:24,988\tINFO config.py:71 -- Setting up process group for: env:// [rank=0, world_size=3]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m 2023-04-05 12:16:27,732\tINFO train_loop_utils.py:300 -- Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m 2023-04-05 12:16:27,733\tINFO train_loop_utils.py:347 -- Wrapping provided model in DDP.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m 2023-04-05 12:16:27,748\tINFO train_loop_utils.py:300 -- Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m 2023-04-05 12:16:27,749\tINFO train_loop_utils.py:347 -- Wrapping provided model in DDP.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m 2023-04-05 12:16:27,793\tINFO train_loop_utils.py:300 -- Moving model to device: cuda:0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m 2023-04-05 12:16:27,794\tINFO train_loop_utils.py:347 -- Wrapping provided model in DDP.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Current time: 2023-04-05 12:16:28 (running for 00:00:08.78)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Memory usage on this node: 7.3/31.0 GiB\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Resources requested: 1.0/8 CPUs, 3.0/4 GPUs, 0.0/26.62 GiB heap, 0.0/11.82 GiB objects (0.0/4.0 accelerator_type:A10G)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Result logdir: /home/ray/ray_results/TorchTrainer_2023-04-05_12-16-18\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+----------+---------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | Trial name               | status   | loc           |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m |--------------------------+----------+---------------|\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | TorchTrainer_580a4_00000 | RUNNING  | 10.0.62.72:81 |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+----------+---------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.304860  [    0/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.307808  [    0/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.320101  [    0/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.307238  [ 2100/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.293579  [ 2100/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.306897  [ 2100/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.302743  [ 4200/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.282233  [ 4200/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.299897  [ 4200/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.294896  [ 6300/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.275237  [ 6300/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.271520  [ 6300/20000]\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Current time: 2023-04-05 12:16:33 (running for 00:00:13.78)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Memory usage on this node: 7.3/31.0 GiB\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Resources requested: 1.0/8 CPUs, 3.0/4 GPUs, 0.0/26.62 GiB heap, 0.0/11.82 GiB objects (0.0/4.0 accelerator_type:A10G)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Result logdir: /home/ray/ray_results/TorchTrainer_2023-04-05_12-16-18\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+----------+---------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | Trial name               | status   | loc           |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m |--------------------------+----------+---------------|\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | TorchTrainer_580a4_00000 | RUNNING  | 10.0.62.72:81 |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+----------+---------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.285721  [ 8400/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.252979  [ 8400/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.278272  [ 8400/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.280991  [10500/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.270969  [10500/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.278611  [10500/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.265640  [12600/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.272897  [12600/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.235484  [12600/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.253513  [14700/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.257851  [14700/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.248024  [14700/20000]\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Current time: 2023-04-05 12:16:38 (running for 00:00:18.78)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Memory usage on this node: 7.3/31.0 GiB\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Resources requested: 1.0/8 CPUs, 3.0/4 GPUs, 0.0/26.62 GiB heap, 0.0/11.82 GiB objects (0.0/4.0 accelerator_type:A10G)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Result logdir: /home/ray/ray_results/TorchTrainer_2023-04-05_12-16-18\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+----------+---------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | Trial name               | status   | loc           |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m |--------------------------+----------+---------------|\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | TorchTrainer_580a4_00000 | RUNNING  | 10.0.62.72:81 |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+----------+---------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.217603  [16800/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.262365  [16800/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.248789  [16800/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.223202  [18900/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.206393  [18900/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.211032  [18900/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m Test Error: \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m  Accuracy: 42.8%, Avg loss: 2.217338 \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.249361  [    0/20000]\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Result for TorchTrainer_580a4_00000:\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   _time_this_iter_s: 15.582162141799927\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   _timestamp: 1680722200\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   _training_iteration: 1\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   date: 2023-04-05_12-16-40\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   experiment_id: 33b15b3e374d40bf9191d7f3bcd739a4\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   hostname: ray-642dbfaf53943c280b964b09-ray-worker-1\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   iterations_since_restore: 1\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   loss: 2.2162900990660086\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   node_ip: 10.0.62.72\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   pid: 81\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   time_since_restore: 17.66356611251831\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   time_this_iter_s: 17.66356611251831\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   time_total_s: 17.66356611251831\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   timestamp: 1680722200\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   training_iteration: 1\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   trial_id: 580a4_00000\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   warmup_time: 0.004072427749633789\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m Test Error: \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m  Accuracy: 43.5%, Avg loss: 2.216290 \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.247011  [    0/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m Test Error: \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m  Accuracy: 42.3%, Avg loss: 2.220369 \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.285431  [    0/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.220853  [ 2100/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.185533  [ 2100/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.230207  [ 2100/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.184681  [ 4200/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.228905  [ 4200/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.233757  [ 4200/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.180643  [ 6300/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.153893  [ 6300/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.237352  [ 6300/20000]\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Current time: 2023-04-05 12:16:45 (running for 00:00:26.46)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Memory usage on this node: 7.3/31.0 GiB\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Resources requested: 1.0/8 CPUs, 3.0/4 GPUs, 0.0/26.62 GiB heap, 0.0/11.82 GiB objects (0.0/4.0 accelerator_type:A10G)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Result logdir: /home/ray/ray_results/TorchTrainer_2023-04-05_12-16-18\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+----------+---------------+--------+------------------+---------+--------------+---------------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | Trial name               | status   | loc           |   iter |   total time (s) |    loss |   _timestamp |   _time_this_iter_s |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m |--------------------------+----------+---------------+--------+------------------+---------+--------------+---------------------|\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | TorchTrainer_580a4_00000 | RUNNING  | 10.0.62.72:81 |      1 |          17.6636 | 2.21629 |   1680722200 |             15.5822 |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+----------+---------------+--------+------------------+---------+--------------+---------------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.205136  [ 8400/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.126923  [ 8400/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.168885  [ 8400/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.193332  [10500/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.192569  [10500/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.203835  [10500/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.107750  [12600/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.203768  [12600/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.186172  [12600/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.170832  [14700/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.162050  [14700/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.130144  [14700/20000]\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Current time: 2023-04-05 12:16:50 (running for 00:00:31.47)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Memory usage on this node: 7.3/31.0 GiB\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Resources requested: 1.0/8 CPUs, 3.0/4 GPUs, 0.0/26.62 GiB heap, 0.0/11.82 GiB objects (0.0/4.0 accelerator_type:A10G)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Result logdir: /home/ray/ray_results/TorchTrainer_2023-04-05_12-16-18\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+----------+---------------+--------+------------------+---------+--------------+---------------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | Trial name               | status   | loc           |   iter |   total time (s) |    loss |   _timestamp |   _time_this_iter_s |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m |--------------------------+----------+---------------+--------+------------------+---------+--------------+---------------------|\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | TorchTrainer_580a4_00000 | RUNNING  | 10.0.62.72:81 |      1 |          17.6636 | 2.21629 |   1680722200 |             15.5822 |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+----------+---------------+--------+------------------+---------+--------------+---------------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.154281  [16800/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.188313  [16800/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.060977  [16800/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.042051  [18900/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.106447  [18900/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.080023  [18900/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m Test Error: \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m  Accuracy: 45.4%, Avg loss: 2.088018 \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m Test Error: \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m  Accuracy: 45.1%, Avg loss: 2.089403 \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m Test Error: \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m  Accuracy: 44.9%, Avg loss: 2.095751 \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Result for TorchTrainer_580a4_00000:\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   _time_this_iter_s: 13.036194324493408\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   _timestamp: 1680722213\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   _training_iteration: 2\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   date: 2023-04-05_12-16-53\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   experiment_id: 33b15b3e374d40bf9191d7f3bcd739a4\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   hostname: ray-642dbfaf53943c280b964b09-ray-worker-1\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   iterations_since_restore: 2\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   loss: 2.088018142952109\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   node_ip: 10.0.62.72\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   pid: 81\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   time_since_restore: 30.695749759674072\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   time_this_iter_s: 13.032183647155762\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   time_total_s: 30.695749759674072\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   timestamp: 1680722213\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   training_iteration: 2\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   trial_id: 580a4_00000\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   warmup_time: 0.004072427749633789\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.153733  [    0/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.165399  [    0/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.255980  [    0/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.120059  [ 2100/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.121294  [ 2100/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.016407  [ 2100/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.147567  [ 4200/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.124730  [ 4200/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.025773  [ 4200/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.157100  [ 6300/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 1.987288  [ 6300/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.020745  [ 6300/20000]\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Current time: 2023-04-05 12:16:58 (running for 00:00:39.48)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Memory usage on this node: 7.3/31.0 GiB\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Resources requested: 1.0/8 CPUs, 3.0/4 GPUs, 0.0/26.62 GiB heap, 0.0/11.82 GiB objects (0.0/4.0 accelerator_type:A10G)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Result logdir: /home/ray/ray_results/TorchTrainer_2023-04-05_12-16-18\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+----------+---------------+--------+------------------+---------+--------------+---------------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | Trial name               | status   | loc           |   iter |   total time (s) |    loss |   _timestamp |   _time_this_iter_s |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m |--------------------------+----------+---------------+--------+------------------+---------+--------------+---------------------|\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | TorchTrainer_580a4_00000 | RUNNING  | 10.0.62.72:81 |      2 |          30.6957 | 2.08802 |   1680722213 |             13.0362 |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+----------+---------------+--------+------------------+---------+--------------+---------------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.024641  [ 8400/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 1.913772  [ 8400/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.081972  [ 8400/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.093151  [10500/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.077097  [10500/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.063389  [10500/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.087752  [12600/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.089378  [12600/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 1.892066  [12600/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.042980  [14700/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 1.944373  [14700/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.019192  [14700/20000]\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Current time: 2023-04-05 12:17:03 (running for 00:00:44.48)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Memory usage on this node: 7.3/31.0 GiB\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Resources requested: 1.0/8 CPUs, 3.0/4 GPUs, 0.0/26.62 GiB heap, 0.0/11.82 GiB objects (0.0/4.0 accelerator_type:A10G)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Result logdir: /home/ray/ray_results/TorchTrainer_2023-04-05_12-16-18\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+----------+---------------+--------+------------------+---------+--------------+---------------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | Trial name               | status   | loc           |   iter |   total time (s) |    loss |   _timestamp |   _time_this_iter_s |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m |--------------------------+----------+---------------+--------+------------------+---------+--------------+---------------------|\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | TorchTrainer_580a4_00000 | RUNNING  | 10.0.62.72:81 |      2 |          30.6957 | 2.08802 |   1680722213 |             13.0362 |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+----------+---------------+--------+------------------+---------+--------------+---------------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.035537  [16800/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 1.824287  [16800/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.091877  [16800/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 1.904265  [18900/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 1.939211  [18900/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 1.809882  [18900/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m Test Error: \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m  Accuracy: 44.8%, Avg loss: 1.926125 \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m Test Error: \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m  Accuracy: 45.8%, Avg loss: 1.915166 \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Result for TorchTrainer_580a4_00000:\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   _time_this_iter_s: 12.569870948791504\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   _timestamp: 1680722226\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   _training_iteration: 3\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   date: 2023-04-05_12-17-06\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   experiment_id: 33b15b3e374d40bf9191d7f3bcd739a4\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   hostname: ray-642dbfaf53943c280b964b09-ray-worker-1\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   iterations_since_restore: 3\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   loss: 1.9140727819886598\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   node_ip: 10.0.62.72\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   pid: 81\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   time_since_restore: 43.26283669471741\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   time_this_iter_s: 12.567086935043335\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   time_total_s: 43.26283669471741\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   timestamp: 1680722226\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   training_iteration: 3\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   trial_id: 580a4_00000\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   warmup_time: 0.004072427749633789\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m Test Error: \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m  Accuracy: 45.9%, Avg loss: 1.914073 \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.035195  [    0/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 2.220520  [    0/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 2.049868  [    0/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 1.958451  [ 2100/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 1.783050  [ 2100/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.002716  [ 2100/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 1.989301  [ 4200/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 1.828064  [ 4200/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.044990  [ 4200/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 2.039591  [ 6300/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 1.801990  [ 6300/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 1.821581  [ 6300/20000]\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Current time: 2023-04-05 12:17:11 (running for 00:00:52.05)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Memory usage on this node: 7.3/31.0 GiB\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Resources requested: 1.0/8 CPUs, 3.0/4 GPUs, 0.0/26.62 GiB heap, 0.0/11.82 GiB objects (0.0/4.0 accelerator_type:A10G)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Result logdir: /home/ray/ray_results/TorchTrainer_2023-04-05_12-16-18\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+----------+---------------+--------+------------------+---------+--------------+---------------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | Trial name               | status   | loc           |   iter |   total time (s) |    loss |   _timestamp |   _time_this_iter_s |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m |--------------------------+----------+---------------+--------+------------------+---------+--------------+---------------------|\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | TorchTrainer_580a4_00000 | RUNNING  | 10.0.62.72:81 |      3 |          43.2628 | 1.91407 |   1680722226 |             12.5699 |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+----------+---------------+--------+------------------+---------+--------------+---------------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 1.839998  [ 8400/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 1.674169  [ 8400/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 1.923163  [ 8400/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 1.970450  [10500/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 1.955276  [10500/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 1.914990  [10500/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 1.962064  [12600/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 1.944887  [12600/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 1.674916  [12600/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 1.912499  [14700/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 1.778233  [14700/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 1.845008  [14700/20000]\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Current time: 2023-04-05 12:17:16 (running for 00:00:57.05)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Memory usage on this node: 7.3/31.0 GiB\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Resources requested: 1.0/8 CPUs, 3.0/4 GPUs, 0.0/26.62 GiB heap, 0.0/11.82 GiB objects (0.0/4.0 accelerator_type:A10G)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Result logdir: /home/ray/ray_results/TorchTrainer_2023-04-05_12-16-18\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Number of trials: 1/1 (1 RUNNING)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+----------+---------------+--------+------------------+---------+--------------+---------------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | Trial name               | status   | loc           |   iter |   total time (s) |    loss |   _timestamp |   _time_this_iter_s |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m |--------------------------+----------+---------------+--------+------------------+---------+--------------+---------------------|\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | TorchTrainer_580a4_00000 | RUNNING  | 10.0.62.72:81 |      3 |          43.2628 | 1.91407 |   1680722226 |             12.5699 |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+----------+---------------+--------+------------------+---------+--------------+---------------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 1.608098  [16800/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 1.984904  [16800/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 1.878340  [16800/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m loss: 1.700707  [18900/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m loss: 1.790661  [18900/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m loss: 1.602710  [18900/20000]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m Test Error: \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m  Accuracy: 47.3%, Avg loss: 1.741285 \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=110, ip=10.0.62.72)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Result for TorchTrainer_580a4_00000:\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   _time_this_iter_s: 12.591781377792358\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   _timestamp: 1680722239\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   _training_iteration: 4\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   date: 2023-04-05_12-17-19\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   done: false\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   experiment_id: 33b15b3e374d40bf9191d7f3bcd739a4\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   hostname: ray-642dbfaf53943c280b964b09-ray-worker-1\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   iterations_since_restore: 4\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   loss: 1.7373216264652755\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   node_ip: 10.0.62.72\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   pid: 81\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   time_since_restore: 55.84689283370972\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   time_this_iter_s: 12.58405613899231\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   time_total_s: 55.84689283370972\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   timestamp: 1680722239\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   training_iteration: 4\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   trial_id: 580a4_00000\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   warmup_time: 0.004072427749633789\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m Test Error: \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m  Accuracy: 47.2%, Avg loss: 1.737322 \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=299)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m Test Error: \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m  Accuracy: 46.2%, Avg loss: 1.754438 \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=80, ip=10.0.38.132)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m 2023-04-05 12:17:22,402\tWARNING util.py:220 -- The `process_trial_save` operation took 1.645 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m 2023-04-05 12:17:22,402\tWARNING trial_runner.py:950 -- Consider turning off forced head-worker trial checkpoint syncs by setting sync_on_checkpoint=False. Note that this may result in faulty trial restoration if a failure occurs while the checkpoint is being synced from the worker to the head node.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Result for TorchTrainer_580a4_00000:\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   _time_this_iter_s: 12.591781377792358\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   _timestamp: 1680722239\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   _training_iteration: 4\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   date: 2023-04-05_12-17-19\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   done: true\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   experiment_id: 33b15b3e374d40bf9191d7f3bcd739a4\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   experiment_tag: '0'\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   hostname: ray-642dbfaf53943c280b964b09-ray-worker-1\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   iterations_since_restore: 4\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   loss: 1.7373216264652755\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   node_ip: 10.0.62.72\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   pid: 81\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   time_since_restore: 55.84689283370972\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   time_this_iter_s: 12.58405613899231\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   time_total_s: 55.84689283370972\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   timestamp: 1680722239\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   timesteps_since_restore: 0\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   training_iteration: 4\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   trial_id: 580a4_00000\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   warmup_time: 0.004072427749633789\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m 2023-04-05 12:17:23,257\tINFO tune.py:758 -- Total run time: 64.67 seconds (63.70 seconds for the tuning loop).\n",
      "2023-04-05 19:17:23,269\tERROR checkpoint_manager.py:133 -- The requested checkpoint is not available on this node, most likely because you are using Ray client or disabled checkpoint synchronization. To avoid this, enable checkpoint synchronization to cloud storage by specifying a `SyncConfig`. The checkpoint may be available on a different node - please check this location on worker nodes: /home/ray/ray_results/TorchTrainer_2023-04-05_12-16-18/TorchTrainer_580a4_00000_0_2023-04-05_12-16-20/checkpoint_-00001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Current time: 2023-04-05 12:17:23 (running for 00:01:03.70)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Memory usage on this node: 4.1/31.0 GiB\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Resources requested: 0/8 CPUs, 0/4 GPUs, 0.0/26.62 GiB heap, 0.0/11.82 GiB objects (0.0/4.0 accelerator_type:A10G)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Result logdir: /home/ray/ray_results/TorchTrainer_2023-04-05_12-16-18\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Number of trials: 1/1 (1 TERMINATED)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+------------+---------------+--------+------------------+---------+--------------+---------------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | Trial name               | status     | loc           |   iter |   total time (s) |    loss |   _timestamp |   _time_this_iter_s |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m |--------------------------+------------+---------------+--------+------------------+---------+--------------+---------------------|\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | TorchTrainer_580a4_00000 | TERMINATED | 10.0.62.72:81 |      4 |          55.8469 | 1.73732 |   1680722239 |             12.5918 |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+------------+---------------+--------+------------------+---------+--------------+---------------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m == Status ==\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Current time: 2023-04-05 12:17:23 (running for 00:01:03.70)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Memory usage on this node: 4.1/31.0 GiB\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Using FIFO scheduling algorithm.\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Resources requested: 0/8 CPUs, 0/4 GPUs, 0.0/26.62 GiB heap, 0.0/11.82 GiB objects (0.0/4.0 accelerator_type:A10G)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Result logdir: /home/ray/ray_results/TorchTrainer_2023-04-05_12-16-18\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m Number of trials: 1/1 (1 TERMINATED)\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+------------+---------------+--------+------------------+---------+--------------+---------------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | Trial name               | status     | loc           |   iter |   total time (s) |    loss |   _timestamp |   _time_this_iter_s |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m |--------------------------+------------+---------------+--------+------------------+---------+--------------+---------------------|\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m | TorchTrainer_580a4_00000 | TERMINATED | 10.0.62.72:81 |      4 |          55.8469 | 1.73732 |   1680722239 |             12.5918 |\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m +--------------------------+------------+---------------+--------+------------------+---------+--------------+---------------------+\n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "\u001b[2m\u001b[36m(TunerInternal pid=209)\u001b[0m \n",
      "Last result: {'loss': 1.7373216264652755, '_timestamp': 1680722239, '_time_this_iter_s': 12.591781377792358, '_training_iteration': 4, 'time_this_iter_s': 12.58405613899231, 'done': True, 'timesteps_total': None, 'episodes_total': None, 'training_iteration': 4, 'trial_id': '580a4_00000', 'experiment_id': '33b15b3e374d40bf9191d7f3bcd739a4', 'date': '2023-04-05_12-17-19', 'timestamp': 1680722239, 'time_total_s': 55.84689283370972, 'pid': 81, 'hostname': 'ray-642dbfaf53943c280b964b09-ray-worker-1', 'node_ip': '10.0.62.72', 'config': {}, 'time_since_restore': 55.84689283370972, 'timesteps_since_restore': 0, 'iterations_since_restore': 4, 'warmup_time': 0.004072427749633789, 'experiment_tag': '0'}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--address\", required=False, type=str, help=\"the address to use for Ray\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-workers\",\n",
    "        \"-n\",\n",
    "        type=int,\n",
    "        default=3,\n",
    "        help=\"Sets number of workers for training.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use-gpu\", action=\"store_true\", default=True, help=\"Enables GPU training\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--smoke-test\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Finish quickly for testing.\",\n",
    "    )\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    import ray\n",
    "    if ray.is_initialized() == False:\n",
    "        print(\"Connecting to Ray cluster...\")\n",
    "        service_host = os.environ[\"RAY_HEAD_SERVICE_HOST\"]\n",
    "        service_port = os.environ[\"RAY_HEAD_SERVICE_PORT\"]\n",
    "        ray.util.connect(f\"{service_host}:{service_port}\")\n",
    "\n",
    "\n",
    "    # if args.smoke_test:\n",
    "    #     # 2 workers + 1 for trainer.\n",
    "    #     ray.init(num_cpus=3)\n",
    "    #     train_fashion_mnist()\n",
    "    # else:\n",
    "    # ray.init(address=args.address)\n",
    "    train_fashion_mnist(num_workers=args.num_workers, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b3bce2-55d7-4425-9023-64228886c9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
